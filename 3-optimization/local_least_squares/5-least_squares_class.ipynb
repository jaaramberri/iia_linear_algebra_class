{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9d4fb-aac0-4366-8f85-5c08de7739e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c51007-76d4-4487-851d-c872f1c321da",
   "metadata": {},
   "source": [
    "# Least-Squares Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196f90c-38ad-4c33-b279-a9ca81d158c9",
   "metadata": {},
   "source": [
    "Lets solve a Linear Regression model with Least-Squares. \n",
    "\n",
    "\n",
    "- **Model**:\n",
    "  \n",
    "  The linear regression model is defined as \n",
    "  \\begin{equation}\n",
    "  h_\\boldsymbol{\\theta} = \\mathbf{x}^T \\boldsymbol{\\theta}\n",
    "  \\end{equation}\n",
    "where $\\boldsymbol{\\theta}$ is the parameter vector and $\\mathbf{x}$ is the feature vector.\n",
    "  \n",
    "  \n",
    "**Cost Functional**:\n",
    "\n",
    "  The least squares cost function is given by\n",
    "  \\begin{equation}\n",
    "    J(\\boldsymbol{\\theta}) = \\frac{1}{2m} \\sum_{i=1}^m \\Big(r^{(i)}(\\boldsymbol{\\theta}) \\Big)^2= \\frac{1}{2m} \\sum_{i=1}^m \\Big( h_\\boldsymbol{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)} \\Big)^2\n",
    "  \\end{equation}\n",
    "  where \n",
    "- $m$: the number of training examples.\n",
    "- $r^{(i)}= h_\\boldsymbol{\\theta}(x^{(i)}) - y^{(i)}$ is the residual.\n",
    "- $\\mathbf{x}^{(i)}$ is the $i$-th input.\n",
    "- $y^{(i)}$ is the corresponding target value.\n",
    "\n",
    "**Gradient**\n",
    "\n",
    "  The derivative of $J(\\boldsymbol{\\theta})$ with respect to the parameter $\\theta_j$ is (Least-Squares):\n",
    "  \\begin{equation}\n",
    "    \\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\Big( h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)} \\Big) x_j^{(i)}.\n",
    "  \\end{equation}\n",
    "  In vectorized form, the gradient is written as:\n",
    "  \\begin{equation} \n",
    "  \\nabla J(\\boldsymbol{\\theta}) = \\frac{1}{m} X^T \\big( X\\boldsymbol{\\theta} - \\boldsymbol{y} \\big) = \\frac{1}{m} X^T  \\boldsymbol{r},\n",
    "  \\end{equation}\n",
    "  where $X$ is the matrix with data, $\\boldsymbol{y}$ is the vector of target values and $\\boldsymbol{r}$ is the vector of residuals.\n",
    "\n",
    "  ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcb719-4a78-4b9b-81e3-2ef502c08927",
   "metadata": {},
   "source": [
    "Given a set of labeled data $\\boldsymbol{x},\\boldsymbol{y}$, we will implement 5 optimization algorithms to compute the Least-Squares solution. **Remark**: We know it has an analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe0079-8afe-4614-910d-751e0bd7e76a",
   "metadata": {},
   "source": [
    "- We use our Linear Regression function to compute the analytical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db88d0-b7cb-4c46-9fb0-511d5c3b3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, Y):\n",
    "    X_mean = X.mean()\n",
    "    Y_mean = Y.mean()\n",
    "    numerator = np.sum((X - X_mean) * (Y - Y_mean))\n",
    "    denominator = np.sum((X - X_mean) ** 2)\n",
    "    w = numerator / denominator\n",
    "    b = Y_mean - w * X_mean\n",
    "    return np.array([[b], [w]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa9aeb-a817-4e33-9dd9-4599f272cd5b",
   "metadata": {},
   "source": [
    "- We generate some sinthetic data to test each optimizer individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f0900-363b-4856-832b-b3ba4834046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    m_examples_individual = 100\n",
    "\n",
    "    b_true_individual = 4.0\n",
    "    w_true_individual = 3.0 \n",
    "\n",
    "    # Generate synthetic data\n",
    "    X_individual = 2 * np.random.rand(m_examples_individual, 1)\n",
    "    y_individual = b_true_individual + w_true_individual * X_individual + np.random.randn(m_examples_individual, 1)\n",
    "\n",
    "    # Add bias term to X to create design matrix\n",
    "    X_b_individual = np.c_[np.ones((m_examples_individual, 1)), X_individual]\n",
    "\n",
    "    # Compute analytic solution\n",
    "    theo_theta_individual = linear_regression(X_individual, y_individual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21403636-a999-4f6e-91f7-42d7d6bfd133",
   "metadata": {},
   "source": [
    "# Exercise 1: Gradient Descent\n",
    "\n",
    "Implement a function named `gradient_descent` that implements an easy version of the gradient descent optimization algorithm for linear regression.\n",
    "\n",
    "**INPUTS**:\n",
    "\n",
    "- **X**: The training data matrix where each row represents a training example.\n",
    "- **y**: The corresponding target values.\n",
    "- **theta**: The initial parameters (weights) to be optimized.\n",
    "- **learning_rate**: The step size used for updating the parameters.\n",
    "- **iterations**: The maximum number of iterations to run.\n",
    "\n",
    "---\n",
    "**INITIALIZATION**:\n",
    "- Determine the number of training examples, denoted by $m$.\n",
    "- The parameters $\\boldsymbol{\\theta}$ are initialized with the provided initial guess.\n",
    "\n",
    "---\n",
    "**MAIN LOOP** (for a given number of iterations):\n",
    "- **Prediction Computation**: Compute the predictions using the linear model:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\text{predictions} = X \\boldsymbol{\\theta}\n",
    "  \\end{equation}\n",
    "  \n",
    "- **Error Calculation**: Compute the residuals as the difference between the predictions and the actual target values:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\text{residuals} = \\text{predictions} - y\n",
    "  \\end{equation}\n",
    "  \n",
    "- **Gradient Computation**: Compute the gradient of the cost function (mean squared error) with respect to $\\boldsymbol{\\theta}$:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\nabla J(\\boldsymbol{\\theta}) = \\frac{1}{m} X^T (\\text{residuals})\n",
    "  \\end{equation}\n",
    "  \n",
    "- **Parameter Update**: Update the parameters using the gradient descent update rule:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\text{learning\\_rate} \\cdot \\nabla J(\\boldsymbol{\\theta})\n",
    "  \\end{equation}\n",
    "\n",
    "---  \n",
    "**OUTPUT**:\n",
    "The function returns the optimized parameter vector $\\boldsymbol{\\theta}$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cf1c4-477a-4fae-b45c-b26581d65bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Gradient Descent\n",
    "# -------------------------\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    # Number of training examples\n",
    "    m = len(y)\n",
    "    \n",
    "    # Loop over the specified number of iterations\n",
    "    for _ in range(iterations):\n",
    "        \n",
    "        # Compute predictions by taking the dot product of X and w\n",
    "\n",
    "                \n",
    "        # Compute residuals as the difference between predictions and y\n",
    "\n",
    "                \n",
    "        # Compute the gradient: (X^T * residuals) divided by the number of examples\n",
    "\n",
    "                \n",
    "        # Update the weights by subtracting the product of learning rate and gradient\n",
    "\n",
    "                \n",
    "    # Return the optimized weights\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3b34f-e8f1-4a3f-9ccd-cf7b5c4a09c7",
   "metadata": {},
   "source": [
    "- Test the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ded4a-0e03-45b0-8e00-4bea5721ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize parameters randomly if not provided\n",
    "    theta_initial_ind = np.array([[1.0], [1.0]])\n",
    "    learning_rate_ind = 0.003\n",
    "    nr_iterations_inv = 1000\n",
    "\n",
    "    # Run optimization method\n",
    "    theta_individual = gradient_descent(X_b_individual, y_individual, theta_initial_ind, learning_rate_ind, nr_iterations_inv)\n",
    "\n",
    "    print(\"=============== GD ==================\")\n",
    "    print(\"True Parameters (b, w):\")\n",
    "    print(np.array([[b_true_individual], [w_true_individual]]))\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Theoretical Parameters (b, w):\")\n",
    "    print(theo_theta_individual)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Initial Parameters (b, w):\")\n",
    "    print(theta_initial_ind)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Obtained Parameters (b, w):\")\n",
    "    print(theta_individual)\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "# # OUTPUT: \n",
    "# =============== GD ==================\n",
    "# True Parameters (b, w):\n",
    "# [[4.]\n",
    "#  [3.]]\n",
    "# ===================================\n",
    "\n",
    "# Theoretical Parameters (b, w):\n",
    "# [[4.22215108]\n",
    "#  [2.96846751]]\n",
    "# ===================================\n",
    "\n",
    "# Initial Parameters (b, w):\n",
    "# [[1.]\n",
    "#  [1.]]\n",
    "# ===================================\n",
    "\n",
    "# Obtained Parameters (b, w):\n",
    "# [[3.70807238]\n",
    "#  [3.41498547]]\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbcdf1-6f18-4278-a273-c2cfb1f2059e",
   "metadata": {},
   "source": [
    "# Exercise 2: Stochastic Gradient Descent\n",
    "\n",
    "Implement a function named `stochastic_gradient_descent` that implements the stochastic gradient descent optimization algorithm for linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "**INPUTS**:\n",
    "\n",
    "- **X**: The training data matrix where each row represents a training example.\n",
    "- **y**: The corresponding target values.\n",
    "- **theta**: The initial parameters (weights) to be optimized.\n",
    "- **learning_rate**: The step size for updating the parameters.\n",
    "- **iterations**: The maximum number of iterations to run.\n",
    "\n",
    "---\n",
    "\n",
    "**INITIALIZATION**:\n",
    "- Determine the number of training examples, denoted by $m$.\n",
    "\n",
    "---\n",
    "\n",
    "**MAIN LOOP** (for a given number of iterations):\n",
    "- **Loop over the specified number of iterations.**\n",
    "  - **Inner Loop (over each training example)**:\n",
    "    - Select a random index from the dataset (from $0$ to $m-1$).\n",
    "    - Extract the training example at the random index and reshape it as a row vector, denoted by $\\mathbf{x}_i$.\n",
    "    - Extract the corresponding label $y_i$ for the selected training example.\n",
    "    - **Prediction Computation**: Compute the prediction for the selected example:\n",
    "      \\begin{equation}\n",
    "      \\text{prediction} = \\mathbf{x}_i \\cdot \\boldsymbol{\\theta}\n",
    "      \\end{equation}\n",
    "    - **Residuals Calculation**: Compute the residuals (error) as:\n",
    "      \\begin{equation}\n",
    "      \\text{residuals} = \\text{prediction} - y_i\n",
    "      \\end{equation}\n",
    "    - **Gradient Computation**: Compute the gradient for the selected example:\n",
    "      \\begin{equation}\n",
    "      \\text{gradient} = \\mathbf{x}_i^T \\cdot \\text{residuals}\n",
    "      \\end{equation}\n",
    "    - **Parameter Update**: Update the parameters using the gradient descent update rule:\n",
    "      \\begin{equation}\n",
    "      \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\text{learning\\_rate} \\times \\text{gradient}\n",
    "      \\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "**OUTPUT**:\n",
    "The function returns the optimized parameter vector $\\boldsymbol{\\theta}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e03d8116-05ea-450c-a358-35466cc8d172",
   "metadata": {},
   "source": [
    "- Implement the Stochastic Gradient Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461de474-efb9-4ad7-83bc-360497eb813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Stochastic Gradient Descent\n",
    "# -------------------------\n",
    "def stochastic_gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    # Number of training examples. This way we do it through all of them, we can modify and play with this number.\n",
    "    m = len(y)\n",
    "    \n",
    "    # Loop over the specified number of iterations\n",
    "    for _ in range(iterations):\n",
    "        \n",
    "        # Loop over each training example\n",
    "        for i in range(m):\n",
    "            \n",
    "            # Select a random index from the dataset\n",
    "\n",
    "                        \n",
    "            # Extract the training example at the random index and reshape it as a row vector\n",
    "\n",
    "                        \n",
    "            # Extract the corresponding label for the selected training example\n",
    "\n",
    "                        \n",
    "            # Compute the prediction for the selected training example\n",
    "\n",
    "                        \n",
    "            # Compute the residuals for the selected example\n",
    "\n",
    "                        \n",
    "            # Compute the gradient for the selected example\n",
    "\n",
    "                        \n",
    "            # Update the parameters by subtracting the product of learning rate and gradient\n",
    "\n",
    "                        \n",
    "    # Return the optimized parameters\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b1cb5-bc49-428a-83ad-70e53fae6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize parameters randomly if not provided\n",
    "    theta_initial_ind = np.array([[1.0], [1.0]])\n",
    "    learning_rate_ind = 0.003\n",
    "    nr_iterations_inv = 1000\n",
    "\n",
    "    # Run optimization method\n",
    "    theta_individual = stochastic_gradient_descent(X_b_individual, y_individual, theta_initial_ind, learning_rate_ind, nr_iterations_inv)\n",
    "\n",
    "    print(\"=============== SGD ==================\")\n",
    "    print(\"True Parameters (b, w):\")\n",
    "    print(np.array([[b_true_individual], [w_true_individual]]))\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Theoretical Parameters (b, w):\")\n",
    "    print(theo_theta_individual)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Initial Parameters (b, w):\")\n",
    "    print(theta_initial_ind)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Obtained Parameters (b, w):\")\n",
    "    print(theta_individual)\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "#Â OUTPUT:\n",
    "# =============== SGD ==================\n",
    "# True Parameters (b, w):\n",
    "# [[4.]\n",
    "#  [3.]]\n",
    "# ===================================\n",
    "\n",
    "# Theoretical Parameters (b, w):\n",
    "# [[4.22215108]\n",
    "#  [2.96846751]]\n",
    "# ===================================\n",
    "\n",
    "# Initial Parameters (b, w):\n",
    "# [[1.]\n",
    "#  [1.]]\n",
    "# ===================================\n",
    "\n",
    "# Obtained Parameters (b, w):\n",
    "# [[4.23157379]\n",
    "#  [3.04690077]]\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14608ff3-247e-4eb1-8805-006b16dd074c",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 3: RMSprop Optimizer\n",
    "\n",
    "Implement a function named `rmsprop` that applies the RMSprop optimization algorithm to minimize a cost function in the context of linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "**INPUTS**:\n",
    "\n",
    "- **X**: The training data matrix where each row represents a training example.\n",
    "- **y**: The corresponding target values.\n",
    "- **theta**: The initial parameters (weights) to be optimized.\n",
    "- **learning_rate**: The step size for updating the parameters.\n",
    "- **iterations**: The maximum number of iterations to run.\n",
    "- **beta** (default 0.9): The exponential decay rate for the moving average of squared gradients.\n",
    "- **epsilon** (default 1e-7): A small constant added to the denominator for numerical stability.\n",
    "\n",
    "---\n",
    "\n",
    "**INITIALIZATION**:\n",
    "- Initialize the moving average of squared gradients, denoted by $v_t$, as a zero vector (with the same shape as $\\boldsymbol{\\theta}$).\n",
    "- Determine the number of training examples, denoted by $m$.\n",
    "\n",
    "---\n",
    "\n",
    "**MAIN LOOP** (for a given number of iterations):\n",
    "- **Prediction Computation**:  \n",
    "  Compute the predictions as:\n",
    "  \\begin{equation}\n",
    "  \\text{predictions} = X \\, \\boldsymbol{\\theta}\n",
    "  \\end{equation}\n",
    "  \n",
    "- **Residuals Calculation**:  \n",
    "  Compute the residuals (errors) as the difference between predictions and the actual target values:\n",
    "  \\begin{equation}\n",
    "  \\text{residuals} = \\text{predictions} - y\n",
    "  \\end{equation}\n",
    "  \n",
    "- **Gradient Computation**:  \n",
    "  Compute the gradient of the cost function as:\n",
    "  \\begin{equation}\n",
    "  \\text{gradient} = \\frac{1}{m} X^T \\, \\text{residuals}\n",
    "  \\end{equation}\n",
    "  \n",
    "- **Update Moving Average**:  \n",
    "  Update the exponentially decaying average of the squared gradients:\n",
    "  \\begin{equation}\n",
    "  v_t \\leftarrow \\beta \\, v_t + (1 - \\beta) \\, (\\nabla_t J)^2\n",
    "  \\end{equation}\n",
    "  \n",
    "- **Parameter Update**:  \n",
    "  Update the parameters using the RMSprop rule:\n",
    "  \\begin{equation}\n",
    "  \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\frac{\\text{learning\\_rate}}{\\sqrt{v_t} + \\epsilon} \\, \\text{gradient}\n",
    "  \\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "**OUTPUT**:\n",
    "The function returns the optimized parameter vector $\\boldsymbol{\\theta}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a05bc-a048-4d57-b0e6-2c6821b8ec82",
   "metadata": {},
   "source": [
    "- Implement the rmsprop method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b69fcf-5571-4021-9eb1-b6c3c1c1d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# RMSprop Optimizer\n",
    "# -------------------------\n",
    "def rmsprop(X, y, theta, learning_rate, iterations, beta=0.9, epsilon=1e-7):\n",
    "    # Initialize the moving average of squared gradients with zeros (same shape as theta)\n",
    "    v_t = np.zeros_like(theta)\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = len(y)\n",
    "    \n",
    "    # Loop over the specified number of iterations\n",
    "    for _ in range(iterations):\n",
    "        \n",
    "        # Compute predictions as the dot product of X and theta\n",
    "\n",
    "                \n",
    "        # Compute residuals as the difference between predictions and y\n",
    "\n",
    "                \n",
    "        # Compute the gradient as the dot product of X^T and residuals divided by m\n",
    "\n",
    "                \n",
    "        # Update the exponentially decaying average of squared gradients\n",
    "\n",
    "                \n",
    "        # Update the parameters using the RMSprop update rule\n",
    "\n",
    "                \n",
    "    # Return the optimized parameters\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6704c4-9749-437a-a480-fab1585dcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize parameters randomly if not provided\n",
    "    theta_initial_ind = np.array([[1.0], [1.0]])\n",
    "    learning_rate_ind = 0.003\n",
    "    nr_iterations_inv = 1000\n",
    "\n",
    "    # Run optimization method\n",
    "    theta_individual = rmsprop(X_b_individual, y_individual, theta_initial_ind, learning_rate_ind, nr_iterations_inv)\n",
    "\n",
    "    print(\"=============== RMSPROP==============\")\n",
    "    print(\"True Parameters (b, w):\")\n",
    "    print(np.array([[b_true_individual], [w_true_individual]]))\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Theoretical Parameters (b, w):\")\n",
    "    print(theo_theta_individual)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Initial Parameters (b, w):\")\n",
    "    print(theta_initial_ind)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Obtained Parameters (b, w):\")\n",
    "    print(theta_individual)\n",
    "    print(\"===================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834486e-035a-4157-b878-fb40cbdf56ff",
   "metadata": {},
   "source": [
    "# Exercise 4: Adam\n",
    "\n",
    "Implement a function named `adam` that implements the Adam optimization algorithm\n",
    "\n",
    "In this exercise, you will implement the Adam optimization algorithm for training a machine learning model. Adam is an adaptive learning rate optimization algorithm that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "\n",
    "**INPUTS**:\n",
    "- **X**: The training data matrix where each row represents a training example.\n",
    "- **y**: The corresponding target values.\n",
    "- **theta**: The initial parameters (weights) to be optimized.\n",
    "- **learning_rate**: The step size for the parameter updates.\n",
    "- **iterations**: The total number of iterations for the optimization process.\n",
    "- **beta1** (default 0.9): The exponential decay rate for the first moment estimates.\n",
    "- **beta2** (default 0.999): The exponential decay rate for the second moment estimates.\n",
    "- **epsilon** (default 1e-8): A small constant added to the denominator for numerical stability.\n",
    "\n",
    "---\n",
    "**INITIALIZATION**:\n",
    "  - Initialize the time step counter:  $ t = 0 $\n",
    "  - Initialize the first moment vector, $ m_t $, as zeros (with the same shape as $ \\boldsymbol{\\theta} $).\n",
    "  - Initialize the second moment vector, $ v_t $, as zeros (with the same shape as $ \\boldsymbol{\\theta} $).\n",
    "  - Determine the number of training examples: $ m_{\\text{train}} = \\text{len}(y) $\n",
    "\n",
    "---\n",
    "\n",
    "**MAIN LOOP** (for a given number of iterations):\n",
    "  - For a given number of `iterations`, loop over the training examples.\n",
    "  - For each training example:\n",
    "    - Increment the time step `t`.\n",
    "    - Extract the i-th training example and its label.\n",
    "    - Compute the prediction using the current parameters `theta`.\n",
    "    - Calculate the error as the difference between the prediction and the actual label.\n",
    "    - Compute the gradient for the current example.\n",
    "    - Update the biased first moment estimate `m_t` using the exponential decay factor `beta1`.    $m_t \\leftarrow \\beta_1 \\, m_{t-1} + (1 - \\beta_1) \\, \\text{gradient}$.\n",
    "    - Update the biased second moment estimate `v_t` using the exponential decay factor `beta2`.    $v_t \\leftarrow \\beta_2 \\, v_{t-1} + (1 - \\beta_2) \\, (\\text{gradient})^2$.\n",
    "    - Correct the bias in the first moment estimate to obtain `m_t_hat`.    $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$.\n",
    "    - Correct the bias in the second moment estimate to obtain `v_t_hat`.   $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$.\n",
    "    - Update the parameters `theta` using the Adam update rule:\n",
    "      \\begin{equation}\n",
    "            \\theta_t = \\theta_{t-1} - \\text{learning\\_rate} \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "      \\end{equation}\n",
    "\n",
    "---\n",
    "**OUTPUT**:\n",
    "  - Return the optimized parameters \\boldsymbol{\\theta}.\n",
    "\n",
    "---    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e226a-0535-49d1-85a9-aa2e2043a32c",
   "metadata": {},
   "source": [
    "![title](img/adam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7129e-5144-4970-88c7-94fd8a7e117c",
   "metadata": {},
   "source": [
    "- Implement the ADAM method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022817b-1e13-40d0-b501-fcc936cb78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Adam Optimizer\n",
    "# -------------------------\n",
    "def adam(X, y, theta, learning_rate, iterations, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    # Number of training examples\n",
    "    m_train = len(y)\n",
    "    \n",
    "    # Initialize the first moment vector (m_t) with zeros (same shape as theta)\n",
    "\n",
    "        \n",
    "    # Initialize the second moment vector (v_t) with zeros (same shape as theta)\n",
    "\n",
    "        \n",
    "    # Initialize the time step counter\n",
    "\n",
    "        \n",
    "    # Loop over the specified number of iterations\n",
    "    for _ in range(iterations):\n",
    "        \n",
    "        # Loop over each training example\n",
    "        for i in range(m_train):\n",
    "            \n",
    "            # Increment the time step\n",
    "\n",
    "                        \n",
    "            # Extract the i-th training example and reshape as a row vector\n",
    "\n",
    "                        \n",
    "            # Extract the i-th label\n",
    "\n",
    "                        \n",
    "            # Compute the prediction for the i-th example\n",
    "\n",
    "                        \n",
    "            # Compute the error for the i-th example\n",
    "\n",
    "                        \n",
    "            # Compute the gradient for the i-th example\n",
    "\n",
    "                        \n",
    "            # Update the biased first moment estimate\n",
    "\n",
    "                        \n",
    "            # Update the biased second moment estimate\n",
    "\n",
    "                        \n",
    "            # Compute the bias-corrected first moment estimate\n",
    "\n",
    "                        \n",
    "            # Compute the bias-corrected second moment estimate\n",
    "\n",
    "                        \n",
    "            # Update the parameters using the Adam update rule\n",
    "\n",
    "                        \n",
    "    # Return the optimized parameters\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81767c98-8f5f-4478-8ff2-5e9afcdb236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize parameters randomly if not provided\n",
    "    theta_initial_ind = np.array([[1.0], [1.0]])\n",
    "    learning_rate_ind = 0.003\n",
    "    nr_iterations_inv = 1000\n",
    "\n",
    "    # Run optimization method\n",
    "    theta_individual = adam(X_b_individual, y_individual, theta_initial_ind, learning_rate_ind, nr_iterations_inv)\n",
    "\n",
    "    print(\"=============== ADAM ================\")\n",
    "    print(\"True Parameters (b, w):\")\n",
    "    print(np.array([[b_true_individual], [w_true_individual]]))\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Theoretical Parameters (b, w):\")\n",
    "    print(theo_theta_individual)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Initial Parameters (b, w):\")\n",
    "    print(theta_initial_ind)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Obtained Parameters (b, w):\")\n",
    "    print(theta_individual)\n",
    "    print(\"===================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd8e931-3575-408e-b595-daa1a44126c1",
   "metadata": {},
   "source": [
    "- Implement the BFGS method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbecfa-6238-4647-8d13-b60de7de5d74",
   "metadata": {},
   "source": [
    "# Exercise 5: BFGS\n",
    "\n",
    "Implement a function named `bfgs` that implements a version of the BFGS optimization algorithm specifically adapted for linear least squares problems.\n",
    "\n",
    "---\n",
    "\n",
    "**INPUTS:**\n",
    "\n",
    "- **X**: The training data matrix where each row represents a training example.\n",
    "- **y**: The corresponding target values.\n",
    "- **theta**: The initial parameters (weights) to be optimized.\n",
    "- **learning_rate**: The step size multiplier for the search direction.\n",
    "- **iterations**: The maximum number of iterations to run.\n",
    "- **tol** (optional): A tolerance threshold for the norm of the gradient; if the norm falls below this value, the algorithm stops (default is $1\\times10^{-5}$).\n",
    "\n",
    "---\n",
    "\n",
    "**INITIALIZATION:**\n",
    "\n",
    "- Determine the number of training examples:\n",
    "  \\begin{equation}\n",
    "  m = \\text{len}(y)\n",
    "  \\end{equation}\n",
    "\n",
    "- Compute the initial predictions:\n",
    "  \\begin{equation}\n",
    "  \\text{predictions} = X \\cdot \\boldsymbol{\\theta}\n",
    "  \\end{equation}\n",
    "\n",
    "- Compute the initial residuals:\n",
    "  \\begin{equation}\n",
    "  \\text{residuals} = \\text{predictions} - y\n",
    "  \\end{equation}\n",
    "\n",
    "- Compute the initial gradient:\n",
    "  \\begin{equation}\n",
    "  \\text{gradient} = \\frac{1}{m} X^T \\cdot \\text{residuals}\n",
    "  \\end{equation}\n",
    "\n",
    "- Determine the number of parameters $n$ from the shape of $\\boldsymbol{\\theta}$.\n",
    "\n",
    "- Initialize the inverse Hessian approximation $H$ as the identity matrix of size $n \\times n$:\n",
    "  \\begin{equation}\n",
    "  H = I_{n \\times n}\n",
    "  \\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "**MAIN LOOP** (for a given number of iterations):\n",
    "- **Gradient Computation**: The gradient $\\nabla J_k$ at the current point $x$ is calculated.\n",
    "- **Convergence Check**: If the norm of $\\nabla J_k$ is below the tolerance $tol$, the algorithm terminates.\n",
    "- **Search Direction**: The search direction $p$ is computed as $-H \\cdot \\nabla J_k$ (a Newton-like step).\n",
    "  \n",
    "- **Update Step**: The new point is computed as:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\theta_{\\text{new}} = \\theta + \\alpha p\n",
    "  \\end{equation}\n",
    "  \n",
    "  and the new gradient $g_{\\text{new}}$ is calculated.\n",
    "- **BFGS Update**:\n",
    "  - The changes in position $s = \\theta_{\\text{new}} - \\theta$ and gradient $y = \\nabla J_{\\text{new}} - \\nabla J_k$ are computed.\n",
    "  - A scaling factor is computed as $\\rho = \\frac{1}{y^T s}$.\n",
    "  - The inverse Hessian approximation is updated using:\n",
    "  \n",
    "    \\begin{equation}\n",
    "    H \\leftarrow (I - \\rho s\\,y^T) \\, H \\, (I - \\rho y\\,s^T) + \\rho \\, s\\,s^T\n",
    "    \\end{equation}\n",
    "- **Update $\\theta$**: The current point $\\theta$ is set to $\\theta_{\\text{new}}$.\n",
    "\n",
    "---\n",
    "**OUTPUT**:\n",
    "The function returns the optimized parameter vector $\\boldsymbol{\\theta}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7faba25-2011-4898-9cb1-4c95bd70a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(X, y, theta, learning_rate, iterations): #, tol=1e-5):\n",
    "\n",
    "\n",
    "    m = len(y)                     # Number of training examples\n",
    "    n = len(theta)                 # Determine the number of parameters\n",
    "\n",
    "    # Initialize the Gradient vector\n",
    "\n",
    "    # Compute predictions as the dot product of X and theta\n",
    "\n",
    "            \n",
    "    # Compute residuals as the difference between predictions and y\n",
    "\n",
    "            \n",
    "    # Compute the gradient as the dot product of X^T and residuals divided by m\n",
    "\n",
    "    \n",
    "    # Initialize the inverse Hessian approximation H as the identity matrix.\n",
    "    H = np.eye(n)\n",
    "\n",
    "    # Loop over the specified number of iterations.\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Compute predictions as the dot product of X and theta\n",
    "\n",
    "                \n",
    "        # Compute residuals as the difference between predictions and y\n",
    "\n",
    "                \n",
    "        # Compute the gradient as the dot product of X^T and residuals divided by m\n",
    "\n",
    "        \n",
    "#        # Check for convergence: if the norm of the gradient is below the tolerance, stop.\n",
    "#        if np.linalg.norm(gradient) < tol:\n",
    "#            break\n",
    "\n",
    "        # Compute the search direction: p = -H * g.\n",
    "\n",
    "        \n",
    "        # Update the new point: x_new = x + learning_rate * p.\n",
    "\n",
    "        \n",
    "        # Compute predictions as the dot product of X and theta\n",
    "\n",
    "                \n",
    "        # Compute residuals as the difference between predictions and y\n",
    "\n",
    "                \n",
    "        # Compute the new gradient at the updated point.\n",
    "\n",
    "        \n",
    "        # Compute the difference in position: s = theta_new - theta.\n",
    "\n",
    "        \n",
    "        # Compute the difference in gradient: y = g_new - g.\n",
    "\n",
    "        \n",
    "        # Compute the scaling factor: rho = 1 / (y^T * s).\n",
    "\n",
    "        \n",
    "        # Construct the identity matrix.\n",
    "\n",
    "        \n",
    "        # Update the inverse Hessian approximation H using the BFGS formula:\n",
    "        # H = (I - rho * s * y^T) * H * (I - rho * y * s^T) + rho * s * s^T.\n",
    "\n",
    "        \n",
    "        # Update the current point theta to the new point theta_new.\n",
    "\n",
    "                \n",
    "    # Return the optimized parameters theta.\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f815a-4676-49c9-bec8-d148bccf4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize parameters randomly if not provided\n",
    "    theta_initial_ind = np.array([[1.0], [1.0]])\n",
    "    learning_rate_ind = 0.003\n",
    "    nr_iterations_inv = 1000\n",
    "\n",
    "    # Run optimization method\n",
    "    theta_individual = bfgs(X_b_individual, y_individual, theta_initial_ind, learning_rate_ind, nr_iterations_inv)\n",
    "\n",
    "    print(\"=============== BFGS ================\")\n",
    "    print(\"True Parameters (b, w):\")\n",
    "    print(np.array([[b_true_individual], [w_true_individual]]))\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Theoretical Parameters (b, w):\")\n",
    "    print(theo_theta_individual)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Initial Parameters (b, w):\")\n",
    "    print(theta_initial_ind)\n",
    "    print(\"===================================\\n\")\n",
    "    print(\"Obtained Parameters (b, w):\")\n",
    "    print(theta_individual)\n",
    "    print(\"===================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6728760-b62a-4588-ba33-b6bdddc1e86b",
   "metadata": {},
   "source": [
    "# Exercise 6: Comparison\n",
    "\n",
    "Compare all the methods and compute the MAE and MSE for all of them for a randomly generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c15259-652d-413c-b952-10b5fc12b3c1",
   "metadata": {},
   "source": [
    "We use our Linear Regression function to compute the analytical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc14574-009f-4a4b-9559-5e8ac712ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Mean Squared Error (MSE) cost function\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Define the Mean Absolute Error (MAE) cost function\n",
    "def mean_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Main testing function with optimizer comparison and plot\n",
    "def test_optimizers_with_plot(m_examples=100, b_true=4, w_true=3, theta_initial=None, learning_rate=0.003, iterations=1000):\n",
    "\n",
    "    # Generate synthetic data\n",
    "    X = 2 * np.random.rand(m_examples, 1)\n",
    "    y = b_true + w_true * X + np.random.randn(m_examples, 1)\n",
    "\n",
    "    # Add bias term to X to create design matrix\n",
    "    X_b = np.c_[np.ones((m_examples, 1)), X]\n",
    "\n",
    "    # Compute analytic solution\n",
    "    theo_theta = linear_regression(X, y)\n",
    "\n",
    "    # Initialize parameters randomly if not provided\n",
    "    if theta_initial is None:\n",
    "        theta_initial = np.random.randn(2, 1)\n",
    "\n",
    "    print(\"===================================\")\n",
    "    print(\"True Parameters (b, w):\")\n",
    "    print(np.array([[b_true], [w_true]]))\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "    print(\"Theoretical Parameters (b, w):\")\n",
    "    print(theo_theta)\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "    print(\"Initial Parameters (b, w):\")\n",
    "    print(theta_initial)\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "    # Run optimization methods\n",
    "    theta_gd = gradient_descent(X_b, y, theta_initial.copy(), learning_rate, iterations)\n",
    "    theta_sgd = stochastic_gradient_descent(X_b, y, theta_initial.copy(), learning_rate, iterations)\n",
    "    theta_rmsprop = rmsprop(X_b, y, theta_initial.copy(), learning_rate, iterations)\n",
    "    theta_adam = adam(X_b, y, theta_initial.copy(), learning_rate, iterations)\n",
    "    theta_bfgs = bfgs(X_b, y, theta_initial.copy(), learning_rate, iterations)\n",
    "\n",
    "    # Compute predictions\n",
    "    preds_gd = np.dot(X_b, theta_gd)\n",
    "    preds_sgd = np.dot(X_b, theta_sgd)\n",
    "    preds_rmsprop = np.dot(X_b, theta_rmsprop)\n",
    "    preds_adam = np.dot(X_b, theta_adam)\n",
    "    preds_bfgs = np.dot(X_b, theta_bfgs)\n",
    "\n",
    "    # Print results\n",
    "    header = f\"{'Method':<35}{'Optimized Theta (b,w)':<35}{'MSE':<20}{'MAE':<20}\"\n",
    "    print(header)\n",
    "    print(\"=\" * len(header))\n",
    "    print(f\"{'Gradient Descent':<35}{str(theta_gd.flatten()):<35}{mean_squared_error(y, preds_gd):<20.5f}{mean_error(y, preds_gd):<20.5f}\")\n",
    "    print(f\"{'Stochastic Gradient Descent':<35}{str(theta_sgd.flatten()):<35}{mean_squared_error(y, preds_sgd):<20.5f}{mean_error(y, preds_sgd):<20.5f}\")\n",
    "    print(f\"{'RMSprop':<35}{str(theta_rmsprop.flatten()):<35}{mean_squared_error(y, preds_rmsprop):<20.5f}{mean_error(y, preds_rmsprop):<20.5f}\")\n",
    "    print(f\"{'Adam':<35}{str(theta_adam.flatten()):<35}{mean_squared_error(y, preds_adam):<20.5f}{mean_error(y, preds_adam):<20.5f}\")\n",
    "    print(f\"{'BFGS for Least Squares':<35}{str(theta_bfgs.flatten()):<35}{mean_squared_error(y, preds_bfgs):<20.5f}{mean_error(y, preds_bfgs):<20.5f}\")\n",
    "\n",
    "    # Plot original data\n",
    "    plt.scatter(X, y, color='blue', label='Data')\n",
    "\n",
    "    # X values for line plotting\n",
    "    X_range = np.linspace(X.min(), X.max(), 100)\n",
    "\n",
    "    # Predictions for plotting\n",
    "    Y_true = theo_theta[0, 0] + theo_theta[1, 0] * X_range\n",
    "    Y_gd = theta_gd[0, 0] + theta_gd[1, 0] * X_range\n",
    "    Y_sgd = theta_sgd[0, 0] + theta_sgd[1, 0] * X_range\n",
    "    Y_rmsprop = theta_rmsprop[0, 0] + theta_rmsprop[1, 0] * X_range\n",
    "    Y_adam = theta_adam[0, 0] + theta_adam[1, 0] * X_range\n",
    "    Y_bfgs = theta_bfgs[0, 0] + theta_bfgs[1, 0] * X_range\n",
    "\n",
    "    # Plot regression lines\n",
    "    plt.plot(X_range, Y_true, color='black', linewidth=2, label='Analytic Solution')\n",
    "    plt.plot(X_range, Y_gd, color='red', linewidth=2, label='Gradient Descent')\n",
    "    plt.plot(X_range, Y_sgd, color='green', linewidth=2, label='Stochastic GD')\n",
    "    plt.plot(X_range, Y_rmsprop, color='blue', linewidth=2, label='RMSprop')\n",
    "    plt.plot(X_range, Y_adam, color='purple', linewidth=2, label='Adam')\n",
    "    plt.plot(X_range, Y_bfgs, color='orange', linewidth=2, label='BFGS')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Comparison of Linear Regressions with Different Optimizers')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951dbe85-f0da-409b-9c50-d232961e6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of examples\n",
    "m_examples = 100\n",
    "\n",
    "# Define the true parameters for the data generation\n",
    "b_true = 4\n",
    "w_true = 3\n",
    "\n",
    "# Define initial parameters (bias and weight) manually\n",
    "theta_initial = np.array([[1.0], [1.0]])\n",
    "\n",
    "# Define learning rate and number of iterations\n",
    "learning_rate = 0.003\n",
    "iterations = 1000\n",
    "\n",
    "# Call the test function with all parameters defined\n",
    "test_optimizers_with_plot(\n",
    "    m_examples=m_examples,\n",
    "    b_true=b_true,\n",
    "    w_true=w_true,\n",
    "    theta_initial=theta_initial,\n",
    "    learning_rate=learning_rate,\n",
    "    iterations=iterations\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
